#!/usr/bin/env python3

### ---------------------------------------- ###

class author_predictor:

    """
    Residual Convolutional Neural Network for the prediction of authors based on books' text.

    Attributes
    ----------

    input_size : int
        Size of input images in pixels.

    img_train : tf.dataset
        Training set generated by load_images with 'train_set' mode.

    img_val : tf.dataset
        Validation set generated by load_images with 'train_set' mode.

    authors_to_id : dict
        Dictionary matching authors' names to one-hot encoding indexes.

    authors_n : int
        Number of author classes.

    images : tf.dataset
        Images loaded by load_images with 'test_set' or 'single_book' modes.

    img_ids : list
        List of loaded image ids.

    actual_authors : list
        List of true authors of the test set.

    model : tf.model
        Tensorflow model.

    Methods
    -------

    load_images()
        Loads images for training/validation, or testing, or prediction of individual books.

    define_model()
        Inits model.

    conv_block()
        Static method for generating a convolution block.

    save_model_structure()
        Static method for saving the model's structure to file.

    train_model()
        Trains the model.

    save_model()
        Saves the model locally.

    load_model()
        Loads the model from a specified directory.

    predict_individual_books()
        Wrapper for predict_book_author().
        Given a directory with subdirectories containing images of individual books, it predicts the
        author of each book, separately.

    predict_book_author()
        Predicts the author of a book from the loaded images.
        It assumes that all images are from the same book.

    predict_book_set()
        Individually predicts the authors of all loaded images.
        Use for testing set.
    """

    ### ------------------------------------ ###
    ### INIT                                 ###
    ### ------------------------------------ ###
    
    def __init__(self):
        
        pass

    ### ------------------------------------ ###
    ### LOAD IMAGES                          ###
    ### ------------------------------------ ###
    
    def load_images(self, inputs_dir, image_size, batch_size=100, mode='train_set'):

        print('### Loading images')

        self.input_size = image_size

        if mode == 'train_set':

            self.img_train, self.img_val = image_dataset_from_directory(inputs_dir,
                                                                        labels='inferred',
                                                                        label_mode='categorical',
                                                                        class_names=None,
                                                                        color_mode='grayscale',
                                                                        batch_size=batch_size,
                                                                        image_size=(self.input_size, self.input_size),
                                                                        shuffle=True,
                                                                        seed=42,
                                                                        validation_split=0.2,
                                                                        subset='both')

            self.authors_to_id = {c : i for i,c in enumerate(self.img_train.class_names)}

            self.authors_n = len(self.authors_to_id)

        elif mode == 'test_set':

            self.images = image_dataset_from_directory(inputs_dir,
                                                       labels='inferred',
                                                       label_mode='categorical',
                                                       class_names=None,
                                                       color_mode='grayscale',
                                                       batch_size=batch_size,
                                                       image_size=(self.input_size, self.input_size),
                                                       shuffle=False,
                                                       seed=42,
                                                       validation_split=None,
                                                       subset=None)

            self.img_ids = [fp.split('/')[-1].replace('.png', '') for fp in self.images.file_paths]

            self.actual_authors = []
            actual_authors_list = np.array(self.images.class_names)
            for _, labels_batch in self.images:

                self.actual_authors += actual_authors_list[np.argmax(labels_batch, axis=1)].tolist()

            self.actual_authors = np.array(self.actual_authors)

        elif mode == 'single_book':

            self.images = image_dataset_from_directory(inputs_dir,
                                                       labels=None,
                                                       color_mode='grayscale',
                                                       batch_size=batch_size,
                                                       image_size=(self.input_size, self.input_size),
                                                       shuffle=False,
                                                       seed=42,
                                                       validation_split=None,
                                                       subset=None)

            self.img_ids = [fp.split('/')[-1].replace('.png', '') for fp in self.images.file_paths]

        else:

            print('ERROR: unkown mode.')

        print('### All images loaded')
    
    ### ------------------------------------ ###
    ### MODEL DEFINITION                     ###
    ### ------------------------------------ ###

    def define_model(self):

        print('### Building and compiling model')

        # Define inputs
        inputs = Input(shape=(self.input_size, self.input_size, 1), name='input_layer')

        # Rescale inputs
        rescaled_inputs = layers.Rescaling(1./255, name='rescaling')(inputs)

        # Init conv
        conv_features = layers.Conv2D(32, 3, strides=1, padding='valid', kernel_initializer='he_normal', data_format='channels_last')(rescaled_inputs)
        conv_features = layers.BatchNormalization()(conv_features)
        conv_features = layers.LeakyReLU()(conv_features)

        # Pooling layer
        conv_features = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid")(conv_features)

        # Convolution and skip blocks
        for _ in range(3):

            conv_features = self.conv_block(conv_features, 32, 3)

        # Pooling layer
        conv_features = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid")(conv_features)

        # Dropout
        conv_features = layers.Dropout(0.25)(conv_features)
        
        # Layer 4, dense
        flat_1 = layers.Flatten()(conv_features)
        dense_1 = layers.Dense(int(self.authors_n * 4), name='dense_1')(flat_1)
        
        # Layer 5, dense
        flat_2 = layers.Flatten()(dense_1)
        dense_2 = layers.Dense(self.authors_n, name='dense_2')(flat_2)
        
        # Layer 6, softmax
        outputs = layers.Softmax(name='authors_prediction')(dense_2)

        # Define autoencoder model
        self.model = Model(inputs, outputs, name="author_predictor")

        # Compile model
        self.model.compile(optimizer='adam', loss='categorical_crossentropy')

        # Save structure of model to text/image files
        self.save_model_structure(self.model, model_name='book_predictor')

        print('### Model successfully compiled')

    ### ------------------------------------ ###
    
    @staticmethod
    def conv_block(features_in, n_filters, kernel_size):
        
        # First conv2d, 1 x 1
        features_out = layers.Conv2D(n_filters, 1, strides=1, padding='valid', kernel_initializer='he_normal', data_format='channels_last')(features_in)
        features_out = layers.BatchNormalization()(features_out)
        features_out = layers.LeakyReLU()(features_out)
        
        # Second conv2d, kernel_size x kernel_size
        features_out = layers.Conv2D(n_filters, kernel_size, strides=1, padding='valid', kernel_initializer='he_normal', data_format='channels_last')(features_out)
        features_out = layers.BatchNormalization()(features_out)
        features_out = layers.LeakyReLU()(features_out)

        # Third conv2d, 1 x 1
        features_out = layers.Conv2D(n_filters, 1, strides=1, padding='valid', kernel_initializer='he_normal', data_format='channels_last')(features_out)
        features_out = layers.BatchNormalization()(features_out)
        features_out = layers.LeakyReLU()(features_out)
        
        # Residuals
        residuals = layers.Conv2D(n_filters, kernel_size, strides=1, padding='valid', kernel_initializer='he_normal', data_format='channels_last')(features_in)
        
        # Add residuals conv2d, kernel_size x kernel_size
        features_out = layers.add([features_out, residuals])
        
        return features_out
    
    ### ------------------------------------ ###
    
    @staticmethod
    def save_model_structure(model, model_name='model'):
        
        # Write model structure to text file
        model_structure = []
        model.summary(print_fn=lambda x: model_structure.append(x))
        model_structure = '\n'.join(model_structure)

        with open(f'{model_name}.txt', 'w') as output:

            output.write(model_structure)

        # Plot model structure
        #plot_model(model,
        #           to_file=f'{model_name}.png',
        #           show_shapes=True,
        #           show_dtype=False,
        #           show_layer_names=True,
        #           rankdir="TB",
        #           expand_nested=True,
        #           dpi=300,
        #           show_layer_activations=True)

    ### ------------------------------------ ###
    ### MODEL TRAINING/TESTING               ###
    ### ------------------------------------ ###

    def train_model(self, epochs=50, save_model=True):

        print('### Commencing training')

        # Optimize datasets
        self.img_train = self.img_train.cache().prefetch(buffer_size=AUTOTUNE)
        self.img_val = self.img_val.cache().prefetch(buffer_size=AUTOTUNE)

        # Set early stopping callback (use restore_best_weights=False when training with low resources, e.g. on a laptop)
        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=False)
        #early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=True)

        # Train model
        train_history = self.model.fit(self.img_train, epochs=epochs, validation_data=self.img_val, callbacks=[early_stopping])
        
        # Plot loss history
        plt.figure(figsize=(15, 5))
        plt.plot(train_history.history['loss'])
        plt.plot(train_history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        plt.savefig('model_loss.png', bbox_inches='tight', dpi=300)
        plt.close()
        
        # Save model
        if save_model:
            
            self.save_model()

        print('### Model training complete')

    ### ------------------------------------ ###
    ### MODEL SAVE/LOAD                      ###
    ### ------------------------------------ ###

    def load_model(self, models_dir='./'):

        models_dir = abspath(models_dir)

        if 'author_predictor.keras' not in listdir(models_dir) or 'authors_to_id.tsv' not in listdir(models_dir):

            print('ERROR: specified folder does not contain necessary files.')
            return ''

        self.model = load_model(f'{models_dir}/author_predictor.keras', compile=True)

        self.authors_to_id = {line.split('\t')[0] : int(line.split('\t')[1]) for line in open(f'{models_dir}/authors_to_id.tsv', 'r').read().split('\n') if len(line)}

    ### ------------------------------------ ###

    def save_model(self, save_dir='./'):

        # Make save directory
        if exists(save_dir) and save_dir not in ['.', './']:

            save_dir = abspath(save_dir)

            counter = 0
            while exists(f'{save_dir}_{counter}'):

                counter += 1
            
            save_dir = f'{save_dir}_{counter}'
            mkdir(save_dir)

            print(f'WARNING: specified path already exists, saving model to {save_dir} instead')
        
        else:
            
            save_dir = abspath(save_dir)

        print(f'{save_dir}/author_predictor.keras')
        
        # Save model
        self.model.save(f'{save_dir}/author_predictor.keras', overwrite=True, save_format='keras')

        # Save authors_to_id to text file
        output_text = '\n'.join([f'{a}\t{ai}' for a,ai in self.authors_to_id.items()])
        with open('authors_to_id.tsv', 'w') as authors_to_id_file:
            
            authors_to_id_file.write(output_text)

    ### ------------------------------------ ###
    ### MODEL PREDICT                        ###
    ### ------------------------------------ ###

    def predict_individual_books(self, main_dir='./', image_size=128):

        for book_dir in listdir(main_dir):

            try:

                print(f'#### Predicting authorship for {book_dir}')
                
                self.load_images(f'{main_dir}/{book_dir}/', image_size, mode='single_book')
                self.predict_book_author(f'{book_dir}_prediction.tsv')

                print('')

            except:

                continue

    ### ------------------------------------ ###

    def predict_book_author(self, output_name='book_author_prediction.tsv'):

        print('### Predicting book author')

        id_to_authors = {ai : a for a,ai in self.authors_to_id.items()}
        
        probabilities = self.model.predict(self.images, batch_size=100, verbose=0)

        probabilities = probabilities.mean(axis=0)

        probabilities = [(a, p) for a,p in zip(self.authors_to_id.keys(), probabilities)]

        probabilities.sort(key=lambda p: p[1], reverse=True)

        if len(output_name):

            output_text = ['\t'.join(['author', 'probability'])]
            for a,p in probabilities:
                
                output_text.append(f'{a}\t{p}')
            
            output_text = '\n'.join(output_text)
            
            with open(output_name, 'w') as output:
                
                output.write(output_text)

        print('### All done')
        print(f'### Most likely author: {probabilities[0][0]}')
        print(f'### p = {probabilities[0][1]:.3f}')

    ### ------------------------------------ ###

    def predict_book_set(self, output_name='test_set_authors_prediction.tsv'):

        print('### Predicting authors')
        
        id_to_authors = {ai : a for a,ai in self.authors_to_id.items()}

        probabilities = self.model.predict(self.images, batch_size=100, verbose=0)

        predicted_authors = [id_to_authors[idx] for idx in np.argmax(probabilities, axis=1)]
        
        correct_assignment = sum([1 for pa,aa in zip(predicted_authors, self.actual_authors) if pa == aa])

        if len(output_name):

            output_text = ['\t'.join(['image_id', 'most_likely_author', 'actual_author'] + list(self.authors_to_id.keys()))]
            for ii,pa,aa,prob in zip(self.img_ids, predicted_authors, self.actual_authors, probabilities):
                
                output_text.append('\t'.join([ii, pa, aa] + prob.astype(str).tolist()))
            
            output_text = '\n'.join(output_text)
            
            with open(output_name, 'w') as output:
                
                output.write(output_text)

        precision = 100 * correct_assignment / probabilities.shape[0]

        print('### All done')
        print(f'### Correct predictions: {correct_assignment} / {probabilities.shape[0]} ({precision:.2f}%)')

### ------------------MAIN------------------ ###

import matplotlib.pyplot as plt
import numpy as np

from os import listdir, mkdir
from os.path import abspath, exists
from tensorflow.data import AUTOTUNE
from tensorflow.keras import Input, layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.utils import image_dataset_from_directory, plot_model
